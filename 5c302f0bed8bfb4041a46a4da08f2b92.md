---
title: Outlier Identification Methods
author: Lorenzo Drumond
date: 2024-11-22T09:30:55
last: 2024-11-22T09:39:23
zk_id: 5c302f0bed8bfb4041a46a4da08f2b92
tags: #machine_learning #ml #identification #outlier #methods #statistics
---


# Outlier Identification Methods

## Z-score

The Z-score method is a statistically based approach for outlier detection. It computes the standard score, or Z-score, for each data point. It computes how many standard deviations a data point deviates from the mean of the dataset. We then set a threshold for our Z-score, and data points with Z-scores greater than it are considered outliers. An important assumption made by the Z-score method is that your data is normally distributed, making it especially useful for datasets with symmetrical patterns around the mean.

```python
from sklearn.datasets import load_breast_cancer
from scipy import stats

threshold = 2.5
df = load_breast_cancer(as_frame=True).data
z_scores = stats.zscore(df)
outliers = df[abs(z_scores) > threshold]
```

## Local Outlier Factor (LOF)

The Local Outlier Factor (LOF) algorithm calculates a data point’s local density deviation in relation to its neighbors. LOF assigns an anomaly score to each data point, indicating how likely it is to be an outlier. Outliers are points that have a high anomaly score.

LOF calculates the LOF score for each data point by comparing the local density of each data point to the local densities of its neighbors. An outlier is a data point whose local density is significantly lower than that of its neighbors. Because it considers the concept of local density, LOF is useful for datasets with a range of densities or clusters.

```python
from sklearn.datasets import load_breast_cancer
from sklearn.neighbors import LocalOutlierFactor

data = load_breast_cancer(as_frame=True).data
lof = LocalOutlierFactor(n_neighbours=20,contamination=0.1)
outliers = lof.fit_predict(data)
data["LOF"] = outliers
```

## Isolation Forest

The Isolation Forest algorithm is an effective and efficient unsupervised outlier detection tool. It operates by isolating outliers as abnormalities in a random forest structure. Unlike typical decision trees, which divide data into non-overlapping sections, the Isolation Forest method randomly selects features and splits data points until outliers are isolated into individual leaves.

 Isolation Forest assigns an anomaly score to each data point, with lower scores indicating a higher risk of being an outlier. The approach takes advantage of the fact that outliers are expected to have shorter average path lengths in the random forest, making them easier to isolate.

```python
from sklearn.datasets import load_breast_cancer
from sklearn.ensemble import IsolationForest

data = load_breast_cancer(as_frame=True).data
iso = IsolationForest(contamination=0.1)
outliers = iso.fit_predict(data)
data["ISO"] = outliers
```

## DBSCAN

DBSCAN is a density-based clustering technique that can also detect outliers. It gathers data points that are close to each other depending on a distance criterion. Outliers are data points that are far removed from any cluster. DBSCAN defines three types of data points:

- Core Points: Data points within a specified neighborhood of a minimum number of other data points
- Border Points: Data points within the specified neighborhood of a core point but do not have enough neighboring points to be considered core themselves
- Noise Points (Outliers): Data points that are neither core nor border points

The DBSCAN technique does not require a prior specification of the number of clusters, making it ideal for datasets with an unknown number of clusters. It identifies outliers based on their separation from dense data regions.

```python
from sklearn.datasets import load_breast_cancer
from sklearn.cluster import DBSCAN

data = load_breast_cancer(as_frame=True).data
dbscan = DBSCAN()
outliers = dbscan.fit_predict(data[['mean radius']])
data["DBSCAN"] = outliers
```

## Coresets

Coresets use concepts from computational geometry to significantly reduce a dataset’s size while maintaining the original dataset’s statistical properties. This is done by computing coresets for subsections of the entire dataset, then taking unions of pairs of coresets and computing a new coreset until we are left with one coreset that represents our dataset. This approach results in a tree-like structure containing all coresets computed to get the final result, called a streaming tree or a coreset tree.

Coresets use a measure called importance or sensitivity to determine the impact of individual data points on candidate solutions for the given loss function. Higher importance values typically indicate that a data point is likely to be an outlier.

```python
import numpy as np
from dataheroes import CoresetTreeServiceDTC

data = load_breast_cancer()
X = np.array(data.data)
y = np.array(data.target)
tree = CoresetTreeServiceDTC(optimized_for = "cleaning")
tree = tree.build(X=X,y=y)
result = tree.get_important_samples(20)
tree.remove_samples(result['idx'])
```

# References

- https://dataheroes.ai/blog/outlier-detection-methods-every-data-enthusiast-must-know/
